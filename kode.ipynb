{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomState(MT19937) at 0x7FEBAE81FD40"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "check_random_state(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data balancing\n",
    "def perf_smote(x, y, seed):\n",
    "    x_reshaped = x.reshape(x.shape[0], -1)\n",
    "\n",
    "    smote = SMOTE(random_state=seed)\n",
    "\n",
    "    x_resampled, y_resampled = smote.fit_resample(x_reshaped, y)\n",
    "    x_resampled = x_resampled.reshape(x_resampled.shape[0], *x.shape[1:])\n",
    "\n",
    "    return x_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_Days</th>\n",
       "      <th>Status</th>\n",
       "      <th>Drug</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ascites</th>\n",
       "      <th>Hepatomegaly</th>\n",
       "      <th>Spiders</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Copper</th>\n",
       "      <th>Alk_Phos</th>\n",
       "      <th>SGOT</th>\n",
       "      <th>Tryglicerides</th>\n",
       "      <th>Platelets</th>\n",
       "      <th>Prothrombin</th>\n",
       "      <th>Stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2221</td>\n",
       "      <td>C</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>18499</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.5</td>\n",
       "      <td>149.0</td>\n",
       "      <td>4.04</td>\n",
       "      <td>227.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>52.70</td>\n",
       "      <td>57.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1230</td>\n",
       "      <td>C</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>19724</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>0.5</td>\n",
       "      <td>219.0</td>\n",
       "      <td>3.93</td>\n",
       "      <td>22.0</td>\n",
       "      <td>663.0</td>\n",
       "      <td>45.00</td>\n",
       "      <td>75.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4184</td>\n",
       "      <td>C</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>11839</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.5</td>\n",
       "      <td>320.0</td>\n",
       "      <td>3.54</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>122.45</td>\n",
       "      <td>80.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2090</td>\n",
       "      <td>D</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>16467</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.7</td>\n",
       "      <td>255.0</td>\n",
       "      <td>3.74</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>77.50</td>\n",
       "      <td>58.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2105</td>\n",
       "      <td>D</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>21699</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.9</td>\n",
       "      <td>486.0</td>\n",
       "      <td>3.54</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>108.50</td>\n",
       "      <td>109.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   N_Days Status     Drug    Age Sex Ascites Hepatomegaly Spiders Edema  \\\n",
       "0    2221      C  Placebo  18499   F       N            Y       N     N   \n",
       "1    1230      C  Placebo  19724   M       Y            N       Y     N   \n",
       "2    4184      C  Placebo  11839   F       N            N       N     N   \n",
       "3    2090      D  Placebo  16467   F       N            N       N     N   \n",
       "4    2105      D  Placebo  21699   F       N            Y       N     N   \n",
       "\n",
       "   Bilirubin  Cholesterol  Albumin  Copper  Alk_Phos    SGOT  Tryglicerides  \\\n",
       "0        0.5        149.0     4.04   227.0     598.0   52.70           57.0   \n",
       "1        0.5        219.0     3.93    22.0     663.0   45.00           75.0   \n",
       "2        0.5        320.0     3.54    51.0    1243.0  122.45           80.0   \n",
       "3        0.7        255.0     3.74    23.0    1024.0   77.50           58.0   \n",
       "4        1.9        486.0     3.54    74.0    1052.0  108.50          109.0   \n",
       "\n",
       "   Platelets  Prothrombin  Stage  \n",
       "0      256.0          9.9      1  \n",
       "1      220.0         10.8      2  \n",
       "2      225.0         10.0      2  \n",
       "3      151.0         10.2      2  \n",
       "4      151.0         11.5      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"dataset/dickson_liver_cirrhosis.csv\"\n",
    "TARGET_COL = \"Stage\"\n",
    "\n",
    "data_df = pd.read_csv(DATA_PATH)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Status', 'Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema']\n",
      "{'Status': 3, 'Drug': 2, 'Sex': 2, 'Ascites': 2, 'Hepatomegaly': 2, 'Spiders': 2, 'Edema': 3}\n"
     ]
    }
   ],
   "source": [
    "object_cols=[col for col in data_df.columns if data_df[col].dtype==\"object\"]\n",
    "number_of_unique={col:len(data_df[col].unique()) for col in object_cols}\n",
    "\n",
    "print(object_cols)\n",
    "print(number_of_unique)\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ord = OrdinalEncoder()\n",
    "\n",
    "cols_for_ordinal = [\"Status\", \"Sex\"]\n",
    "cols_for_ohe = [\"Drug\", \"Ascites\", \"Hepatomegaly\", \"Spiders\", \"Edema\"]\n",
    "\n",
    "ord_result  = ord.fit_transform(data_df[cols_for_ordinal])\n",
    "ohe_result = ohe.fit_transform(data_df[cols_for_ohe])\n",
    "\n",
    "columns_ohe = ohe.get_feature_names_out(cols_for_ohe)\n",
    "\n",
    "ord_df = pd.DataFrame(ord_result, columns=cols_for_ordinal)\n",
    "ohe_df = pd.DataFrame(ohe_result.toarray(), columns=columns_ohe)\n",
    "\n",
    "data_df = pd.concat([data_df, ohe_df], axis=1).drop(columns=cols_for_ohe)\n",
    "data_df[cols_for_ordinal] = ord_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "val_perct = 0.2\n",
    "test_perct = 0.1\n",
    "\n",
    "x, y = data_df.drop(TARGET_COL, axis=1).values, data_df[TARGET_COL].values\n",
    "y = y - 1\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "x, y = perf_smote(x, y, random_seed)\n",
    "\n",
    "x = torch.from_numpy(x).float()\n",
    "y = torch.from_numpy(y).long()\n",
    "\n",
    "num_class = len(np.unique(y))\n",
    "\n",
    "print(f\"Number of classes: {num_class}\")\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(x, y, test_size=val_perct, stratify=y, random_state=random_seed)\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=test_perct, stratify=train_y, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.targets[idx]\n",
    "        return x, y\n",
    "\n",
    "bs = 128\n",
    "\n",
    "train_ds = TabDataset(train_x, train_y)\n",
    "val_ds = TabDataset(val_x, val_y)\n",
    "test_ds = TabDataset(test_x, test_y)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=bs, shuffle=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "class AttentiveMLP(nn.Module):\n",
    "    def __init__(self, num_class, input_dim=24):\n",
    "        super(AttentiveMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 16)\n",
    "        self.fc7 = nn.Linear(16, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # x, _ = self.att1(x.unsqueeze(1), x.unsqueeze(1), x.unsqueeze(1))\n",
    "        # x = x.squeeze(1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # x, _ = self.att2(x.unsqueeze(1), x.unsqueeze(1), x.unsqueeze(1))\n",
    "        # x = x.squeeze(1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc7(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 24\n",
    "\n",
    "model = AttentiveMLP(num_class, input_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "Epoch [1/20], Step [10/142], Loss: 1.0921\n",
      "Epoch [1/20], Step [20/142], Loss: 1.0495\n",
      "Epoch [1/20], Step [30/142], Loss: 0.9817\n",
      "Epoch [1/20], Step [40/142], Loss: 1.0002\n",
      "Epoch [1/20], Step [50/142], Loss: 0.9536\n",
      "Epoch [1/20], Step [60/142], Loss: 0.9443\n",
      "Epoch [1/20], Step [70/142], Loss: 0.9582\n",
      "Epoch [1/20], Step [80/142], Loss: 0.9837\n",
      "Epoch [1/20], Step [90/142], Loss: 0.9529\n",
      "Epoch [1/20], Step [100/142], Loss: 0.9382\n",
      "Epoch [1/20], Step [110/142], Loss: 0.9341\n",
      "Epoch [1/20], Step [120/142], Loss: 0.9311\n",
      "Epoch [1/20], Step [130/142], Loss: 0.9355\n",
      "Epoch [1/20], Step [140/142], Loss: 0.8643\n",
      "\n",
      "Train Accuracy: 54.72%\n",
      "Train Loss: 0.98\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "\n",
      "Epoch [2/20], Step [10/142], Loss: 0.9313\n",
      "Epoch [2/20], Step [20/142], Loss: 0.9149\n",
      "Epoch [2/20], Step [30/142], Loss: 0.9863\n",
      "Epoch [2/20], Step [40/142], Loss: 0.9036\n",
      "Epoch [2/20], Step [50/142], Loss: 0.9457\n",
      "Epoch [2/20], Step [60/142], Loss: 0.8773\n",
      "Epoch [2/20], Step [70/142], Loss: 0.8645\n",
      "Epoch [2/20], Step [80/142], Loss: 0.9596\n",
      "Epoch [2/20], Step [90/142], Loss: 0.9208\n",
      "Epoch [2/20], Step [100/142], Loss: 0.8513\n",
      "Epoch [2/20], Step [110/142], Loss: 0.8718\n",
      "Epoch [2/20], Step [120/142], Loss: 0.9024\n",
      "Epoch [2/20], Step [130/142], Loss: 0.8473\n",
      "Epoch [2/20], Step [140/142], Loss: 0.8549\n",
      "\n",
      "Train Accuracy: 63.61%\n",
      "Train Loss: 0.94\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "\n",
      "Epoch [3/20], Step [10/142], Loss: 0.8775\n",
      "Epoch [3/20], Step [20/142], Loss: 0.8899\n",
      "Epoch [3/20], Step [30/142], Loss: 0.8881\n",
      "Epoch [3/20], Step [40/142], Loss: 0.9766\n",
      "Epoch [3/20], Step [50/142], Loss: 0.8681\n",
      "Epoch [3/20], Step [60/142], Loss: 0.8935\n",
      "Epoch [3/20], Step [70/142], Loss: 0.8853\n",
      "Epoch [3/20], Step [80/142], Loss: 0.8453\n",
      "Epoch [3/20], Step [90/142], Loss: 0.8321\n",
      "Epoch [3/20], Step [100/142], Loss: 0.9043\n",
      "Epoch [3/20], Step [110/142], Loss: 0.8509\n",
      "Epoch [3/20], Step [120/142], Loss: 0.8326\n",
      "Epoch [3/20], Step [130/142], Loss: 0.8460\n",
      "Epoch [3/20], Step [140/142], Loss: 0.8620\n",
      "\n",
      "Train Accuracy: 66.94%\n",
      "Train Loss: 0.92\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "\n",
      "Epoch [4/20], Step [10/142], Loss: 0.8985\n",
      "Epoch [4/20], Step [20/142], Loss: 0.9108\n",
      "Epoch [4/20], Step [30/142], Loss: 0.8651\n",
      "Epoch [4/20], Step [40/142], Loss: 0.8678\n",
      "Epoch [4/20], Step [50/142], Loss: 0.7871\n",
      "Epoch [4/20], Step [60/142], Loss: 0.8702\n",
      "Epoch [4/20], Step [70/142], Loss: 0.8142\n",
      "Epoch [4/20], Step [80/142], Loss: 0.8221\n",
      "Epoch [4/20], Step [90/142], Loss: 0.8088\n",
      "Epoch [4/20], Step [100/142], Loss: 0.8430\n",
      "Epoch [4/20], Step [110/142], Loss: 0.9342\n",
      "Epoch [4/20], Step [120/142], Loss: 0.8636\n",
      "Epoch [4/20], Step [130/142], Loss: 0.8334\n",
      "Epoch [4/20], Step [140/142], Loss: 0.8562\n",
      "\n",
      "Train Accuracy: 69.16%\n",
      "Train Loss: 0.90\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "\n",
      "Epoch [5/20], Step [10/142], Loss: 0.8784\n",
      "Epoch [5/20], Step [20/142], Loss: 0.8769\n",
      "Epoch [5/20], Step [30/142], Loss: 0.8168\n",
      "Epoch [5/20], Step [40/142], Loss: 0.8443\n",
      "Epoch [5/20], Step [50/142], Loss: 0.8304\n",
      "Epoch [5/20], Step [60/142], Loss: 0.8243\n",
      "Epoch [5/20], Step [70/142], Loss: 0.8101\n",
      "Epoch [5/20], Step [80/142], Loss: 0.8496\n",
      "Epoch [5/20], Step [90/142], Loss: 0.8165\n",
      "Epoch [5/20], Step [100/142], Loss: 0.8473\n",
      "Epoch [5/20], Step [110/142], Loss: 0.8585\n",
      "Epoch [5/20], Step [120/142], Loss: 0.8645\n",
      "Epoch [5/20], Step [130/142], Loss: 0.8121\n",
      "Epoch [5/20], Step [140/142], Loss: 0.9126\n",
      "\n",
      "Train Accuracy: 69.88%\n",
      "Train Loss: 0.89\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "\n",
      "Epoch [6/20], Step [10/142], Loss: 0.9350\n",
      "Epoch [6/20], Step [20/142], Loss: 0.8683\n",
      "Epoch [6/20], Step [30/142], Loss: 0.8326\n",
      "Epoch [6/20], Step [40/142], Loss: 0.8778\n",
      "Epoch [6/20], Step [50/142], Loss: 0.8664\n",
      "Epoch [6/20], Step [60/142], Loss: 0.8660\n",
      "Epoch [6/20], Step [70/142], Loss: 0.8502\n",
      "Epoch [6/20], Step [80/142], Loss: 0.8760\n",
      "Epoch [6/20], Step [90/142], Loss: 0.8546\n",
      "Epoch [6/20], Step [100/142], Loss: 0.8170\n",
      "Epoch [6/20], Step [110/142], Loss: 0.9043\n",
      "Epoch [6/20], Step [120/142], Loss: 0.7732\n",
      "Epoch [6/20], Step [130/142], Loss: 0.8784\n",
      "Epoch [6/20], Step [140/142], Loss: 0.8503\n",
      "\n",
      "Train Accuracy: 70.53%\n",
      "Train Loss: 0.88\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "\n",
      "Epoch [7/20], Step [10/142], Loss: 0.8178\n",
      "Epoch [7/20], Step [20/142], Loss: 0.8342\n",
      "Epoch [7/20], Step [30/142], Loss: 0.7945\n",
      "Epoch [7/20], Step [40/142], Loss: 0.8050\n",
      "Epoch [7/20], Step [50/142], Loss: 0.7911\n",
      "Epoch [7/20], Step [60/142], Loss: 0.8285\n",
      "Epoch [7/20], Step [70/142], Loss: 0.8024\n",
      "Epoch [7/20], Step [80/142], Loss: 0.7892\n",
      "Epoch [7/20], Step [90/142], Loss: 0.8957\n",
      "Epoch [7/20], Step [100/142], Loss: 0.7624\n",
      "Epoch [7/20], Step [110/142], Loss: 0.8246\n",
      "Epoch [7/20], Step [120/142], Loss: 0.8236\n",
      "Epoch [7/20], Step [130/142], Loss: 0.7876\n",
      "Epoch [7/20], Step [140/142], Loss: 0.8091\n",
      "\n",
      "Train Accuracy: 71.95%\n",
      "Train Loss: 0.87\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "\n",
      "Epoch [8/20], Step [10/142], Loss: 0.8331\n",
      "Epoch [8/20], Step [20/142], Loss: 0.8538\n",
      "Epoch [8/20], Step [30/142], Loss: 0.8440\n",
      "Epoch [8/20], Step [40/142], Loss: 0.8355\n",
      "Epoch [8/20], Step [50/142], Loss: 0.7837\n",
      "Epoch [8/20], Step [60/142], Loss: 0.7787\n",
      "Epoch [8/20], Step [70/142], Loss: 0.8219\n",
      "Epoch [8/20], Step [80/142], Loss: 0.7562\n",
      "Epoch [8/20], Step [90/142], Loss: 0.7883\n",
      "Epoch [8/20], Step [100/142], Loss: 0.8018\n",
      "Epoch [8/20], Step [110/142], Loss: 0.8011\n",
      "Epoch [8/20], Step [120/142], Loss: 0.8211\n",
      "Epoch [8/20], Step [130/142], Loss: 0.7961\n",
      "Epoch [8/20], Step [140/142], Loss: 0.7825\n",
      "\n",
      "Train Accuracy: 72.38%\n",
      "Train Loss: 0.87\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "\n",
      "Epoch [9/20], Step [10/142], Loss: 0.8654\n",
      "Epoch [9/20], Step [20/142], Loss: 0.7769\n",
      "Epoch [9/20], Step [30/142], Loss: 0.8277\n",
      "Epoch [9/20], Step [40/142], Loss: 0.8387\n",
      "Epoch [9/20], Step [50/142], Loss: 0.7952\n",
      "Epoch [9/20], Step [60/142], Loss: 0.8368\n",
      "Epoch [9/20], Step [70/142], Loss: 0.7888\n",
      "Epoch [9/20], Step [80/142], Loss: 0.8628\n",
      "Epoch [9/20], Step [90/142], Loss: 0.8178\n",
      "Epoch [9/20], Step [100/142], Loss: 0.8106\n",
      "Epoch [9/20], Step [110/142], Loss: 0.8135\n",
      "Epoch [9/20], Step [120/142], Loss: 0.8273\n",
      "Epoch [9/20], Step [130/142], Loss: 0.8266\n",
      "Epoch [9/20], Step [140/142], Loss: 0.8196\n",
      "\n",
      "Train Accuracy: 72.76%\n",
      "Train Loss: 0.86\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "\n",
      "Epoch [10/20], Step [10/142], Loss: 0.7870\n",
      "Epoch [10/20], Step [20/142], Loss: 0.8476\n",
      "Epoch [10/20], Step [30/142], Loss: 0.7479\n",
      "Epoch [10/20], Step [40/142], Loss: 0.8127\n",
      "Epoch [10/20], Step [50/142], Loss: 0.8433\n",
      "Epoch [10/20], Step [60/142], Loss: 0.8274\n",
      "Epoch [10/20], Step [70/142], Loss: 0.8402\n",
      "Epoch [10/20], Step [80/142], Loss: 0.7900\n",
      "Epoch [10/20], Step [90/142], Loss: 0.7915\n",
      "Epoch [10/20], Step [100/142], Loss: 0.8102\n",
      "Epoch [10/20], Step [110/142], Loss: 0.7902\n",
      "Epoch [10/20], Step [120/142], Loss: 0.7841\n",
      "Epoch [10/20], Step [130/142], Loss: 0.8042\n",
      "Epoch [10/20], Step [140/142], Loss: 0.7894\n",
      "\n",
      "Train Accuracy: 73.51%\n",
      "Train Loss: 0.86\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "\n",
      "Epoch [11/20], Step [10/142], Loss: 0.8205\n",
      "Epoch [11/20], Step [20/142], Loss: 0.8232\n",
      "Epoch [11/20], Step [30/142], Loss: 0.8233\n",
      "Epoch [11/20], Step [40/142], Loss: 0.7942\n",
      "Epoch [11/20], Step [50/142], Loss: 0.8499\n",
      "Epoch [11/20], Step [60/142], Loss: 0.7877\n",
      "Epoch [11/20], Step [70/142], Loss: 0.7916\n",
      "Epoch [11/20], Step [80/142], Loss: 0.8170\n",
      "Epoch [11/20], Step [90/142], Loss: 0.8430\n",
      "Epoch [11/20], Step [100/142], Loss: 0.7928\n",
      "Epoch [11/20], Step [110/142], Loss: 0.8866\n",
      "Epoch [11/20], Step [120/142], Loss: 0.8833\n",
      "Epoch [11/20], Step [130/142], Loss: 0.7776\n",
      "Epoch [11/20], Step [140/142], Loss: 0.7399\n",
      "\n",
      "Train Accuracy: 74.08%\n",
      "Train Loss: 0.85\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "\n",
      "Epoch [12/20], Step [10/142], Loss: 0.8721\n",
      "Epoch [12/20], Step [20/142], Loss: 0.7735\n",
      "Epoch [12/20], Step [30/142], Loss: 0.7451\n",
      "Epoch [12/20], Step [40/142], Loss: 0.8346\n",
      "Epoch [12/20], Step [50/142], Loss: 0.7578\n",
      "Epoch [12/20], Step [60/142], Loss: 0.8481\n",
      "Epoch [12/20], Step [70/142], Loss: 0.8171\n",
      "Epoch [12/20], Step [80/142], Loss: 0.7682\n",
      "Epoch [12/20], Step [90/142], Loss: 0.8154\n",
      "Epoch [12/20], Step [100/142], Loss: 0.7994\n",
      "Epoch [12/20], Step [110/142], Loss: 0.7585\n",
      "Epoch [12/20], Step [120/142], Loss: 0.8161\n",
      "Epoch [12/20], Step [130/142], Loss: 0.8935\n",
      "Epoch [12/20], Step [140/142], Loss: 0.7842\n",
      "\n",
      "Train Accuracy: 74.33%\n",
      "Train Loss: 0.85\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "\n",
      "Epoch [13/20], Step [10/142], Loss: 0.8134\n",
      "Epoch [13/20], Step [20/142], Loss: 0.7955\n",
      "Epoch [13/20], Step [30/142], Loss: 0.7625\n",
      "Epoch [13/20], Step [40/142], Loss: 0.8393\n",
      "Epoch [13/20], Step [50/142], Loss: 0.7935\n",
      "Epoch [13/20], Step [60/142], Loss: 0.7517\n",
      "Epoch [13/20], Step [70/142], Loss: 0.8489\n",
      "Epoch [13/20], Step [80/142], Loss: 0.7735\n",
      "Epoch [13/20], Step [90/142], Loss: 0.8289\n",
      "Epoch [13/20], Step [100/142], Loss: 0.7792\n",
      "Epoch [13/20], Step [110/142], Loss: 0.7898\n",
      "Epoch [13/20], Step [120/142], Loss: 0.7804\n",
      "Epoch [13/20], Step [130/142], Loss: 0.7627\n",
      "Epoch [13/20], Step [140/142], Loss: 0.7221\n",
      "\n",
      "Train Accuracy: 75.17%\n",
      "Train Loss: 0.84\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "\n",
      "Epoch [14/20], Step [10/142], Loss: 0.8355\n",
      "Epoch [14/20], Step [20/142], Loss: 0.8345\n",
      "Epoch [14/20], Step [30/142], Loss: 0.8330\n",
      "Epoch [14/20], Step [40/142], Loss: 0.7836\n",
      "Epoch [14/20], Step [50/142], Loss: 0.8133\n",
      "Epoch [14/20], Step [60/142], Loss: 0.7672\n",
      "Epoch [14/20], Step [70/142], Loss: 0.7432\n",
      "Epoch [14/20], Step [80/142], Loss: 0.8372\n",
      "Epoch [14/20], Step [90/142], Loss: 0.7336\n",
      "Epoch [14/20], Step [100/142], Loss: 0.8220\n",
      "Epoch [14/20], Step [110/142], Loss: 0.7829\n",
      "Epoch [14/20], Step [120/142], Loss: 0.8017\n",
      "Epoch [14/20], Step [130/142], Loss: 0.8086\n",
      "Epoch [14/20], Step [140/142], Loss: 0.8052\n",
      "\n",
      "Train Accuracy: 75.07%\n",
      "Train Loss: 0.84\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "\n",
      "Epoch [15/20], Step [10/142], Loss: 0.7361\n",
      "Epoch [15/20], Step [20/142], Loss: 0.7730\n",
      "Epoch [15/20], Step [30/142], Loss: 0.7405\n",
      "Epoch [15/20], Step [40/142], Loss: 0.7715\n",
      "Epoch [15/20], Step [50/142], Loss: 0.8036\n",
      "Epoch [15/20], Step [60/142], Loss: 0.7928\n",
      "Epoch [15/20], Step [70/142], Loss: 0.8009\n",
      "Epoch [15/20], Step [80/142], Loss: 0.7463\n",
      "Epoch [15/20], Step [90/142], Loss: 0.7666\n",
      "Epoch [15/20], Step [100/142], Loss: 0.8108\n",
      "Epoch [15/20], Step [110/142], Loss: 0.7894\n",
      "Epoch [15/20], Step [120/142], Loss: 0.8358\n",
      "Epoch [15/20], Step [130/142], Loss: 0.7337\n",
      "Epoch [15/20], Step [140/142], Loss: 0.7693\n",
      "\n",
      "Train Accuracy: 75.58%\n",
      "Train Loss: 0.84\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "\n",
      "Epoch [16/20], Step [10/142], Loss: 0.7295\n",
      "Epoch [16/20], Step [20/142], Loss: 0.8251\n",
      "Epoch [16/20], Step [30/142], Loss: 0.7694\n",
      "Epoch [16/20], Step [40/142], Loss: 0.7701\n",
      "Epoch [16/20], Step [50/142], Loss: 0.8119\n",
      "Epoch [16/20], Step [60/142], Loss: 0.8006\n",
      "Epoch [16/20], Step [70/142], Loss: 0.7437\n",
      "Epoch [16/20], Step [80/142], Loss: 0.7428\n",
      "Epoch [16/20], Step [90/142], Loss: 0.7951\n",
      "Epoch [16/20], Step [100/142], Loss: 0.8042\n",
      "Epoch [16/20], Step [110/142], Loss: 0.7475\n",
      "Epoch [16/20], Step [120/142], Loss: 0.7417\n",
      "Epoch [16/20], Step [130/142], Loss: 0.7930\n",
      "Epoch [16/20], Step [140/142], Loss: 0.8019\n",
      "\n",
      "Train Accuracy: 76.10%\n",
      "Train Loss: 0.83\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "\n",
      "Epoch [17/20], Step [10/142], Loss: 0.7998\n",
      "Epoch [17/20], Step [20/142], Loss: 0.7781\n",
      "Epoch [17/20], Step [30/142], Loss: 0.8132\n",
      "Epoch [17/20], Step [40/142], Loss: 0.8438\n",
      "Epoch [17/20], Step [50/142], Loss: 0.8381\n",
      "Epoch [17/20], Step [60/142], Loss: 0.7622\n",
      "Epoch [17/20], Step [70/142], Loss: 0.8018\n",
      "Epoch [17/20], Step [80/142], Loss: 0.8364\n",
      "Epoch [17/20], Step [90/142], Loss: 0.7614\n",
      "Epoch [17/20], Step [100/142], Loss: 0.8048\n",
      "Epoch [17/20], Step [110/142], Loss: 0.8095\n",
      "Epoch [17/20], Step [120/142], Loss: 0.7870\n",
      "Epoch [17/20], Step [130/142], Loss: 0.7955\n",
      "Epoch [17/20], Step [140/142], Loss: 0.7652\n",
      "\n",
      "Train Accuracy: 75.17%\n",
      "Train Loss: 0.83\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "\n",
      "Epoch [18/20], Step [10/142], Loss: 0.7505\n",
      "Epoch [18/20], Step [20/142], Loss: 0.7856\n",
      "Epoch [18/20], Step [30/142], Loss: 0.7716\n",
      "Epoch [18/20], Step [40/142], Loss: 0.8327\n",
      "Epoch [18/20], Step [50/142], Loss: 0.7495\n",
      "Epoch [18/20], Step [60/142], Loss: 0.8094\n",
      "Epoch [18/20], Step [70/142], Loss: 0.7480\n",
      "Epoch [18/20], Step [80/142], Loss: 0.8137\n",
      "Epoch [18/20], Step [90/142], Loss: 0.7158\n",
      "Epoch [18/20], Step [100/142], Loss: 0.8317\n",
      "Epoch [18/20], Step [110/142], Loss: 0.8101\n",
      "Epoch [18/20], Step [120/142], Loss: 0.8022\n",
      "Epoch [18/20], Step [130/142], Loss: 0.7269\n",
      "Epoch [18/20], Step [140/142], Loss: 0.7678\n",
      "\n",
      "Train Accuracy: 76.24%\n",
      "Train Loss: 0.83\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "\n",
      "Epoch [19/20], Step [10/142], Loss: 0.8069\n",
      "Epoch [19/20], Step [20/142], Loss: 0.8061\n",
      "Epoch [19/20], Step [30/142], Loss: 0.8450\n",
      "Epoch [19/20], Step [40/142], Loss: 0.8486\n",
      "Epoch [19/20], Step [50/142], Loss: 0.7785\n",
      "Epoch [19/20], Step [60/142], Loss: 0.6950\n",
      "Epoch [19/20], Step [70/142], Loss: 0.8681\n",
      "Epoch [19/20], Step [80/142], Loss: 0.8075\n",
      "Epoch [19/20], Step [90/142], Loss: 0.7717\n",
      "Epoch [19/20], Step [100/142], Loss: 0.7835\n",
      "Epoch [19/20], Step [110/142], Loss: 0.7925\n",
      "Epoch [19/20], Step [120/142], Loss: 0.7421\n",
      "Epoch [19/20], Step [130/142], Loss: 0.7702\n",
      "Epoch [19/20], Step [140/142], Loss: 0.7508\n",
      "\n",
      "Train Accuracy: 76.91%\n",
      "Train Loss: 0.83\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "\n",
      "Epoch [20/20], Step [10/142], Loss: 0.7929\n",
      "Epoch [20/20], Step [20/142], Loss: 0.7227\n",
      "Epoch [20/20], Step [30/142], Loss: 0.7416\n",
      "Epoch [20/20], Step [40/142], Loss: 0.7713\n",
      "Epoch [20/20], Step [50/142], Loss: 0.7172\n",
      "Epoch [20/20], Step [60/142], Loss: 0.8009\n",
      "Epoch [20/20], Step [70/142], Loss: 0.7931\n",
      "Epoch [20/20], Step [80/142], Loss: 0.7930\n",
      "Epoch [20/20], Step [90/142], Loss: 0.8024\n",
      "Epoch [20/20], Step [100/142], Loss: 0.7166\n",
      "Epoch [20/20], Step [110/142], Loss: 0.7300\n",
      "Epoch [20/20], Step [120/142], Loss: 0.7642\n",
      "Epoch [20/20], Step [130/142], Loss: 0.7181\n",
      "Epoch [20/20], Step [140/142], Loss: 0.7267\n",
      "\n",
      "Train Accuracy: 76.90%\n",
      "Train Loss: 0.83\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "print_every = 10\n",
    "\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "\n",
    "total_step = len(train_dl)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    print(f'Epoch {epoch+1}\\n')\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        pred = torch.argmax(outputs, dim=1)\n",
    "        correct += torch.sum(pred == labels).item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        if (i+1) % print_every == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss / total_step)\n",
    "    print(f\"\\nTrain Accuracy: {train_acc[-1]:.2f}%\")\n",
    "    print(f\"Train Loss: {np.mean(train_loss):.2f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 12\n",
    "print_every = 10\n",
    "valid_loss_min = np.Inf\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    running_loss = 0.0\n",
    "    # scheduler.step(epoch)\n",
    "    correct = 0\n",
    "    total=0\n",
    "    print(f'Epoch {epoch}\\n')\n",
    "    for batch_idx, (data_, target_) in enumerate(train_loader):\n",
    "        #data_, target_ = data_.to(device), target_.to(device)# on GPU\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(data_)\n",
    "        loss = criterion(outputs, target_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        _,pred = torch.max(outputs, dim=1)\n",
    "        correct += torch.sum(pred==target_).item()\n",
    "        total += target_.size(0)\n",
    "        if (batch_idx) % 20 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
    "    train_acc.append(100 * correct / total)\n",
    "    train_loss.append(running_loss/total_step)\n",
    "    print(f'\\ntrain loss: {np.mean(train_loss):.4f}, train acc: {(100 * correct / total):.4f}')\n",
    "    batch_loss = 0\n",
    "    total_t=0\n",
    "    correct_t=0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data_t, target_t in (validation_loader):\n",
    "            #data_t, target_t = data_t.to(device), target_t.to(device)# on GPU\n",
    "            outputs_t = model(data_t)\n",
    "            loss_t = criterion(outputs_t, target_t)\n",
    "            batch_loss += loss_t.item()\n",
    "            _,pred_t = torch.max(outputs_t, dim=1)\n",
    "            correct_t += torch.sum(pred_t==target_t).item()\n",
    "            total_t += target_t.size(0)\n",
    "        val_acc.append(100 * correct_t / total_t)\n",
    "        val_loss.append(batch_loss/len(validation_loader))\n",
    "        network_learned = batch_loss < valid_loss_min\n",
    "        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t / total_t):.4f}\\n')\n",
    "        # Saving the best weight \n",
    "        if network_learned:\n",
    "            valid_loss_min = batch_loss\n",
    "            torch.save(model.state_dict(), 'model_classification_tutorial.pt')\n",
    "            print('Detected network improvement, saving current model')\n",
    "    model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
